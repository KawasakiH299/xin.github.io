<!DOCTYPE html>
<html lang="en"><head>
    <title>My New Hugo Site</title>
    <meta content="text/html;charset=utf-8" http-equiv="Content-Type">
    <meta content="utf-8" http-equiv="encoding">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="format-detection" content="telephone=no" />
    <meta name="theme-color" content="#000084" />
    <link rel="icon" href="https://xin.github.io/favicon.ico">
    <link rel="canonical" href="https://xin.github.io">
    
    
</head>
<body>
<nav class="navbar navbar-inverse navbar-fixed-top">
    <div class="navbar-inner">
        <div class="container">
            <button type="button" class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse"></button>
            <a class="brand" href="https://xin.github.io">My New Hugo Site</a>
            <div class="nav-collapse collapse">
                <ul class="nav">
                    
                    
                </ul>
            </div>
        </div>
    </div>
</nav><div id="content" class="container">

<div class="row-fluid navmargin">
    <div class="page-header">
        <h1>决策树 - Tue, Jul 4, 2023</h1>
    </div>
    <p class="lead"></p>
    <p>实例</p>
<p>hunt 算法是许多决策树算法的基础，包括 ID3、C4.5、CART等。Hunt 算法通过递归方式建立决策树</p>
<p>决策树算法有多个变体和扩展，以下是其中一些常见的决策树算法：</p>
<ol>
<li>
<p>ID3（Iterative Dichotomiser 3）：ID3是最早的决策树算法之一，它使用信息增益（Information Gain）作为特征选择准则。但ID3存在一个问题，就是对具有较多取值的特征有偏好。</p>
<pre tabindex="0"><code>信息增益准则对可取值数目较多的属性有所偏好，为减少这种偏好可能带来的不利影响，C4.5算法采用信息增益率来选择最优划分属性

选取信息增益大的属性作为分裂节点
</code></pre></li>
<li>
<p>C4.5：C4.5是ID3的改进版本，它使用信息增益比（Gain Ratio）作为特征选择准则，解决了ID3对具有较多取值的特征的偏好问题。此外，C4.5还能处理缺失值，并支持数值型特征的离散化。</p>
</li>
</ol>
<pre tabindex="0"><code>C4.5算法选择具有最大信息增益比的特征作为当前节点的分裂准则
</code></pre><ol>
<li>CART（Classification and Regression Trees）：CART是一种用于分类和回归的决策树算法。对于分类问题，CART使用基尼指数（Gini Index）作为特征选择准则；对于回归问题，CART使用平方误差最小化作为特征选择准则。</li>
</ol>
<pre tabindex="0"><code>基尼指数衡量了数据集的不纯度或混乱程度。基尼指数越小，表示数据集的纯度越高，样本的类别分布越集中,
在CART算法中，我们希望选择基尼指数较小的特征作为分裂准则
</code></pre><ol>
<li>RandomForest（随机森林）：随机森林是一种集成学习方法，它通过构建多个决策树来进行分类或回归。每个决策树由随机抽样得到的训练集构建，最后的预测结果由多个决策树的预测结果进行集成。</li>
<li>GBDT（Gradient Boosting Decision Tree）：GBDT是一种迭代的决策树算法，它通过逐步提升每棵树的拟合能力来提高整体模型的性能。在GBDT中，每棵树都试图纠正上一棵树的残差，从而逐步减小预测误差。</li>
</ol>

    <h4><a href="https://xin.github.io">Back to Home</a></h4>
</div>


        </div><footer class="container">
    <hr class="soften">
    <p>
&copy; 

    

<span id="thisyear">2023</span>


</p>
    <p class="text-center">
        
        
        
        
        
    </p>
</footer>

</body><link rel="stylesheet" href="/css/bootstrap.css">
<link rel="stylesheet" href="/css/bootstrap-responsive.css">
<link rel="stylesheet" href="/css/style.css">

<script src="/js/jquery.js"></script>
<script src="/js/bootstrap-386.js"></script>
<script src="/js/bootstrap-transition.js"></script>
<script src="/js/bootstrap-alert.js"></script>
<script src="/js/bootstrap-modal.js"></script>
<script src="/js/bootstrap-dropdown.js"></script>
<script src="/js/bootstrap-scrollspy.js"></script>
<script src="/js/bootstrap-tab.js"></script>
<script src="/js/bootstrap-tooltip.js"></script>
<script src="/js/bootstrap-popover.js"></script>
<script src="/js/bootstrap-button.js"></script>
<script src="/js/bootstrap-collapse.js"></script>
<script src="/js/bootstrap-carousel.js"></script>
<script src="/js/bootstrap-typeahead.js"></script>
<script src="/js/bootstrap-affix.js"></script>
<script>
    _386 = { 
        fastLoad: null ,
        onePass: null , 
        speedFactor: null 
    };

    
    function ThisYear() {
        document.getElementById('thisyear').innerHTML = new Date().getFullYear();
    };
</script>
</html>
